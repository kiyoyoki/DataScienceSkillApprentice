{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f762e12d",
   "metadata": {},
   "source": [
    "# 単回帰分析において，最小二乗法，回帰係数，標準誤差，決定係数を理解し，モデルを構築する(2022/02/01)\n",
    "---\n",
    "## 概要\n",
    "---\n",
    "単回帰分析(regression analysis)について調べ，実際にモデルを構築する．その際，最小二乗法・回帰係数・標準誤差・決定係数についても理解する．\n",
    "\n",
    "## 単回帰分析\n",
    "---\n",
    "「気温が観測できたとき，その日のアイスの売り上げが知りたい」「賃貸マンションの床面積が分かるとき，その家賃を知りたい」といったように，ある値から関連のある別の値を予測したいという期待は数多く存在する．そういった場合に利用できる分析アプローチの一つが単回帰分析である．\n",
    "\n",
    "単回帰分析では，$D=\\{(x_1,y_1),...,(x_n,y_n)\\}$といった2変量のデータセットが標本としてある場合に，その関係性を尤もらしく説明できる直線にフィッティングする．すなわち,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y = \\beta + \\alpha x\n",
    "\\end{eqnarray}\n",
    "\n",
    "という直線(回帰モデル)を考え，データ点の分布に最も近くなるような係数$\\alpha, \\beta$を求めることが単回帰分析の要旨である．この係数をとりわけ回帰係数と呼ぶ．\n",
    "\n",
    "またこのとき，変数$x$を説明変数，$y$を目的変数と呼ぶ．その名の通り，回帰モデルを構築した後，説明変数の値を用いて目的変数の値を予測していくことになる．\n",
    "\n",
    "なお，単回帰分析においては説明変数が一つだけであるが，もちろん説明変数が複数存在する回帰モデルも存在する．この場合，単回帰分析ではなく重回帰分析と呼び名が変わる．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe27c28",
   "metadata": {},
   "source": [
    "## 最小二乗法\n",
    "---\n",
    "単回帰モデルを構築するためには，係数$\\alpha, \\beta$を推定することが必要であると述べた．得られたデータ$D$をより良く近似するためには，元となるデータと予測との誤差が最小になればよい．つまり，以下のような誤差の二乗和が最小になるように$\\alpha，\\beta$を決める．これを最小二乗法という．\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L = \\sum_{i=1}^{n}[y_i-(\\beta + \\alpha x_i)]^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "ここで$L$は損失(Loss)を表す関数とみなせる．さらに，データ$D$に平均を0とするような中心化を施していた場合，直線の切片項$\\beta=0$となるから，\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L = \\sum_{i=1}^{n}(y_i-\\alpha x_i)^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "この損失関数の値が最小となるよう最適化すればよいことになる．\n",
    "\n",
    "$L$を最小化する係数$\\alpha, \\beta$を導出する方法はいくつかあるが，先に結果を以下に示す：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha &=& \\frac{s_{xy}}{s_x} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2} \\\\\n",
    "\\\\\n",
    "\\beta &=& \\bar{y} - \\alpha\\bar{x} = \\bar{y} - \\frac{s_{xy}}{s_x}\\bar{x}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453f00e",
   "metadata": {},
   "source": [
    "### $\\alpha$の導出\n",
    "説明を簡単にするため，先にデータセットを以下のように中心化する：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "X_i &=& x_i - \\bar{x} \\\\\n",
    "Y_i &=& y_i - \\bar{y}\n",
    "\\end{eqnarray}\n",
    "\n",
    "よって損失関数$L$は以下のようになる：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L = \\sum_{i=1}^{n}(Y_i - \\alpha X_i)^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "この関数を$\\alpha$に関する二次関数とみなしたとき，最小値をとるのは(微分値)=0となるところであるから，(※凸関数であるかの吟味については省略する)\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial \\alpha} &=& \\frac{\\partial}{\\partial \\alpha}\\sum_{i=1}^{n}(Y_i - \\alpha X_i)^2 \\\\\n",
    "&=& \\sum_{i=1}^{n}-2X_i(Y_i - \\alpha X_i) \\\\\n",
    "&=& -2\\sum_{i=1}^{n}X_iY_i + 2\\alpha\\sum_{i=1}^{n}X_i^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "よって，\n",
    "\\begin{eqnarray}\n",
    "-2\\sum_{i=1}^{n}X_iY_i + 2\\alpha\\sum_{i=1}^{n}X_i^2 = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "ゆえ，\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\alpha &=& \\frac{\\sum_{i=1}^{n}X_iY_i}{\\sum_{i=1}^{n}X_i^2} \\\\\\\\\n",
    "&=& \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2} \\\\\\\\\n",
    "&=& \\frac{s_{xy}}{s_x}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa4633",
   "metadata": {},
   "source": [
    "### $\\beta$の導出\n",
    "先の証明で以下のような直線の式が導出できた：\n",
    "\n",
    "\\begin{eqnarray}\n",
    "Y &=& \\alpha X \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "変数変換を元に戻すと，\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y-\\bar{y} &=& \\alpha(x-\\bar{x}) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "より，\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y &=& \\alpha x -\\alpha\\bar{x} + \\bar{y}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "となる．つまり，\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y &=& \\alpha x -\\alpha\\bar{x} + \\bar{y}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "となるから，係数を比較して，\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\beta &=& -\\alpha\\bar{x} + \\bar{y}\\\\\\\\\n",
    "&=& \\bar{y}-\\alpha\\bar{x} \\\\\\\\\n",
    "&=& \\bar{y}-\\frac{s_{xy}}{s_x}\\bar{x}\n",
    "\\end{eqnarray}\n",
    "\n",
    "となる．よってパラメータ$\\beta$も導出できた．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd2bec",
   "metadata": {},
   "source": [
    "## 単回帰モデルの評価\n",
    "---\n",
    "### 決定係数\n",
    "### 標準誤差\n",
    "### p値"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601a0c3",
   "metadata": {},
   "source": [
    "## Pythonを用いたシミュレーション\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa11679",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "---\n",
    "[単回帰分析とは，Albert](https://www.albert2005.co.jp/knowledge/statistics_analysis/multivariate_analysis/single_regression)\n",
    "\n",
    "[27-1.単回帰分析，統計WEB](https://bellcurve.jp/statistics/course/9700.html)\n",
    "\n",
    "[単回帰分析，日経リサーチ](https://www.nikkei-r.co.jp/glossary/id=1644)\n",
    "\n",
    "[高校数学でわかる単回帰分析，おいしい数学](https://hiraocafe.com/note/simple-linear-regression.html)\n",
    "\n",
    "[単回帰分析と重回帰分析，Chainerチュートリアル](https://tutorials.chainer.org/ja/07_Regression_Analysis.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285f086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
